{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EQU0SMkrL1E6"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SET UP\n",
        "extract_all = True\n",
        "extract_specific_years = False\n",
        "years_to_extract = range(2004, 2025)"
      ],
      "metadata": {
        "id": "Jj8-zpb0D5OK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DRNlGynkMD4Z"
      },
      "outputs": [],
      "source": [
        "base_url = 'https://www.motorcycle.com'\n",
        "response = requests.get(f'{base_url}/specs/')\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eG6rXXy_RgZq"
      },
      "outputs": [],
      "source": [
        "brands = soup.find('ul',class_='brands-list order minus-one sl-all-brands-list').find_all('a')\n",
        "brands = [[names, links] for names, links in zip([name.text.strip() for name in brands], [link['href'] for link in brands])]\n",
        "brands = pd.DataFrame(brands, columns=['name', 'link'])\n",
        "brands_to_scrap = ['Aprilia', 'BMW', 'Ducati', 'Kawasaki', 'KTM', 'Triumph', 'CFMoto', 'Honda', 'Indian', 'Yamaha', 'Harley-Davidson', 'Suzuki', 'Benelli', 'Buell', 'Erik Buell Racing', 'GAS GAS', 'Husqvarna', 'Kymco', 'LiveWire', 'Moto Guzzi', 'MV Agusta', 'Piaggio', 'Royal Enfield', 'SSR Motorsports', 'Vespa', 'Zero']\n",
        "brands = brands[brands['name'].isin(brands_to_scrap)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9guE-XdEIpKc"
      },
      "outputs": [],
      "source": [
        "# Looping thru the brands to get all of them\n",
        "if extract_all:\n",
        "  moto_dataset_all = pd.DataFrame()\n",
        "  for index, row in brands.iterrows():\n",
        "    moto_dataset = pd.DataFrame()\n",
        "\n",
        "    brand_name = row['name']\n",
        "    brand_url = row['link']\n",
        "    print(brand_name)\n",
        "\n",
        "    brand_response = requests.get(f'{base_url}{brand_url}')\n",
        "    brand_soup = BeautifulSoup(brand_response.text, 'html.parser')\n",
        "\n",
        "    if brand_soup.find('div', class_='pagination-wrap') is not None:\n",
        "      total_pages = int(brand_soup.find('div', class_='pagination-wrap')['data-total-pages'])+1\n",
        "      total_pages = range(1,total_pages)\n",
        "    else:\n",
        "      total_pages = range(1,2)\n",
        "\n",
        "    # Looping thru the pages inside the brand to get all the motorcycles\n",
        "    for page in total_pages:\n",
        "      time.sleep(1)\n",
        "      print(\"=\"*5, page)\n",
        "\n",
        "      page_response = requests.get(f'{base_url}{brand_url}?page_num={page}')\n",
        "      page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "      card_content = page_soup.find('div', class_='spec-page-feed').find_all('div', 'feed-spec-card')\n",
        "      moto_list_url = [moto_card.find('a', class_='card-link')['href'] for moto_card in card_content]\n",
        "\n",
        "      # Looping thru every motorcycle whitin a page to get all its stats\n",
        "      for moto_url in moto_list_url:\n",
        "\n",
        "        print(\"=\"*10, moto_url)\n",
        "        moto_response = requests.get(f'{base_url}{moto_url}')\n",
        "        moto_soup = BeautifulSoup(moto_response.text, 'html.parser')######\n",
        "\n",
        "        moto_img = moto_soup.find('img', class_='gallery-image sl-gallery-image')['src']\n",
        "        base_specs = {'Brand name': brand_name,\n",
        "                      'Moto image': moto_img,\n",
        "                      'profile link': f'{base_url}{moto_url}'\n",
        "                      }\n",
        "\n",
        "        all_specs_raw = moto_soup.find_all('div', class_='vs-specs-table-row')\n",
        "        all_specs = dict((specs.find('div', 'spec-key bold').text.strip(), specs.find('div', 'spec-value').text.strip()) for specs in all_specs_raw)\n",
        "        all_specs.update(base_specs)\n",
        "\n",
        "        moto_df = pd.DataFrame(all_specs, index=[0])\n",
        "        moto_dataset = pd.concat([moto_dataset, moto_df], ignore_index=True)\n",
        "        moto_dataset_all = pd.concat([moto_dataset_all, moto_df], ignore_index=True)\n",
        "        time.sleep(5)\n",
        "\n",
        "    moto_dataset.to_json(f'/content/drive/My Drive/data scrap/motopy/JSON/{brand_name}.json', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Honda its broken in the website so I had to do something extra for it, and scrap year by year\n",
        "if extract_specific_years:\n",
        "  moto_dataset_all = pd.DataFrame()\n",
        "\n",
        "  for index, row in brands.iterrows():\n",
        "    brand_name = row['name']\n",
        "    brand_url = row['link']\n",
        "    print(brand_name)\n",
        "    years_missing = list(years_to_extract)\n",
        "\n",
        "    for year in years_missing:\n",
        "      moto_dataset = pd.DataFrame()\n",
        "\n",
        "      link = f'{base_url}/specs/{brand_name}/{year}.html'\n",
        "      brand_url = '/'.join(link.split('/')[:-1])\n",
        "\n",
        "      brand_response = requests.get(link)\n",
        "      brand_soup = BeautifulSoup(brand_response.text, 'html.parser')\n",
        "\n",
        "      if brand_soup.find('div', class_='pagination-wrap') is not None:\n",
        "        total_pages = int(brand_soup.find('div', class_='pagination-wrap')['data-total-pages'])+1\n",
        "        total_pages = range(1,total_pages)\n",
        "      else:\n",
        "        total_pages = range(1,2)\n",
        "\n",
        "      # Looping thru the pages inside the brand to get all the motorcycles\n",
        "      for page in total_pages:\n",
        "        time.sleep(1)\n",
        "        print(\"=\"*5, page)\n",
        "\n",
        "        page_response = requests.get(f'{link}?page_num={page}')\n",
        "        page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "        card_content = page_soup.find('div', class_='spec-page-feed').find_all('div', 'feed-spec-card')\n",
        "        moto_list_url = [moto_card.find('a', class_='card-link')['href'] for moto_card in card_content]\n",
        "\n",
        "        # Looping thru every motorcycle whitin a page to get all its stats\n",
        "        for moto_url in moto_list_url:\n",
        "\n",
        "          print(\"=\"*10, moto_url)\n",
        "          moto_response = requests.get(f'{base_url}{moto_url}')\n",
        "          moto_soup = BeautifulSoup(moto_response.text, 'html.parser')\n",
        "\n",
        "          moto_img = moto_soup.find('img', class_='gallery-image sl-gallery-image')['src']\n",
        "          base_specs = {'Brand name': brand_name,\n",
        "                        'Moto image': moto_img,\n",
        "                        'profile link': f'{base_url}{moto_url}'}\n",
        "\n",
        "          all_specs_raw = moto_soup.find_all('div', class_='vs-specs-table-row')\n",
        "          all_specs = dict((specs.find('div', 'spec-key bold').text.strip(), specs.find('div', 'spec-value').text.strip()) for specs in all_specs_raw)\n",
        "          all_specs.update(base_specs)\n",
        "\n",
        "          moto_df = pd.DataFrame(all_specs, index=[0])\n",
        "          moto_dataset = pd.concat([moto_dataset, moto_df], ignore_index=True)\n",
        "          moto_dataset_all = pd.concat([moto_dataset_all, moto_df], ignore_index=True)\n",
        "          time.sleep(2)\n",
        "\n",
        "  moto_dataset_all.to_json(f'/content/drive/My Drive/data scrap/motopy/JSON/{brand_name}-todelete.json', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "WrwIR_3H2KZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ep9A6zYX3r9FW8gyy4sGuWB0l1Md-3BX",
      "authorship_tag": "ABX9TyP8UqdcWFk0LUvMFtkOQuzn"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}